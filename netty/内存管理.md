### netty内存规格
- tiny 0~512B的内存块，16B，32B，48B ... 496B(31种)
- small 512B~8K的内存块 512B,，1024B，2048B，4096B(4种)
- normal 8K~16M的内存块
- huge 大于16M的内存块
### netty内部域的内存分配单位
- chunk 向操作系统申请的最小单位，是page的集合，默认16M
- page 是chunk用于管理的内存单位，8K
- subpage 分配小于8K时，会将一个page划分为多个相同的subpage
### netty内存池架构
![[netty_mem_pool.jpg]]
#### poolarena
数量与cpu核数有关
- 包含两个PoolSubPage数组，分别存储tiny和small类型的内存块。
- 6个poolchunklist，分别存储不同利用率的chunk。
#### poolsubpage
- tiny规格，内存单位最小为16b，按照16b依次递增到496b，31个元素
- small规格，分别为512b，1024b，2048b，4096b四个元素。
#### poolchunklist
共有6个不同利用率的chunklist，分别为：
- qinit 内存使用率为0 - 25%
- q000 内存使用率为1 - 50%
- q025 内存使用率为25 - 75%
- q050 内存使用率为50 - 100%
- q075 内存使用率为75 - 100%
- q100 内存使用率为100%
当某个chunlist中的chunk使用率不再该范围中，就移除到其他范围。
<font color="red">注意：</font>
1. 每个poolchunklist使用率之间有交错，是防止出现某个chunk在其中两个chunklist的使用率中频繁切换，导致需要频繁移动到其他chunklist中。
2. qinit的意义
	1. q000不存在prev指针指向qinit，也就是说当一个chunk内存使用率大于25，就会被移动到q000，而当这个chunk内存使用率小于1时，不会移动到qinit，因为q000的prev为null，所以这个chunk会被回收。
	2. qinit的chunk如果内存使用率小于0，也不会回收，而是可以继续被使用，这样就避免重复创建
#### poolchunk
netty使用伙伴算法将16m分成2048个page，每个page是8k，使用一颗满二叉树表示其内存结构。
该满二叉树的特性为，每个父节点都存在两个子节点，所以一个父节点可以表示两倍子节点的内存块，即第1024节点可表示第2048和第2049节点的内存和是16k。当子节点被使用，同时也会修改父节点的状态为使用中。

#### poolchunkpage
也就是page，poolchunkpage会将8k内存按照申请的大小平均分配。比如申请16b，则分成8192/16=512份。
<font color="red">注意1：</font>如何表示512份数据中，哪几份被使用了？
使用bitmap，一个long 有64位，每一位是0或1即可表示是否被使用，即只需要8个long就可以表示512份小内存块。
<font color="red">注意2：</font> 整个poolchunklist只是内存块的池子，如何快速将可被使用的page提取出来？
当我们申请20b，会被扩到32b，从poolchunk中找到合适的poolsubpage，将其分成8k/32b个小内存块，将这个poolsubpage放到subpagepools数组对应表示32b的位置上，拼接成如下图的链表。当我们再次分配32b时，只需要从subpagepools数组中寻找即可。
#### PoolThreadCache & MemoryRegionCache
分配内存时，先从与当前线程绑定的PoolThreadCache中寻找MemoryRegionCache。
PoolThreadCache内部维护了三大类memroy region cache数组，如下图所示
MemoryRegionCache内部维护了queue，sizeclass(tiny，small，normal)和size(tiny有16b-496b，31种，small有512b，1k，2k，4k，normal有8k，16k，32k)
![[netty_thread_cache.jpg]]
<font color="red">注意1：</font> 为什么只缓存小于等于32k的？
在网络编程中，大多数申请的内存buffer都是小内存，如果对32k以上的page缓存，很可能会瞬间将该page打满，打满的page需要从链表中移除，则缓存也就失去了其价值。同时对大内存块的缓存，会加重GC和内存的负担。

### 内存分配原理
poolthreadcache是线程私有，poolarena是线程共享的
#### 分配page级别
左侧为初始状态，第一次分配8k时，根据算法计算出树深11，从左往右找到可用的2048节点，设置其状态为已满，逐层设置父节点状态为部分使用。
![[netty_mem_8k_1.jpg]]

接下来分配16k，根据算法计算出树深为10，从左往右找，1024节点只剩8k不够用，找到1025节点刚好16k，修改状态为已满，修改子节点状态都为已满。
![[netty_mem_16k_1.jpg]]

接下来再分配一个8k内存，根据算法计算出树深为11，从左往右找，2048已满，找到2049，设置其状态为已满，修改父节点1024的状态从部分可用-》已满，向上类推父节点状态。
![[netty_mem_8k_2.jpg]]
#### 分配subpage级别
- 分配20b，扩到32b
- 优先从poolthreadcache中分配，如果分配失败，走poolarena分配流程
- 从poolarena的tinypoolsubpage数组中找到32b位置的链表，如果有可用的page，则直接分配
- 按照q050 q025 q000 qinit q075的顺序找可用的chunk，因为32b小于8k，所以从数深11层开始找page，找到则分配
- 找不到则创建一个page，按照32b划分，将创建的page关联到满二叉树上，并且关联到tinypoolsubpage数组的链表上
### 内存回收
每8192次使用poolthreadcache进行一次内存分配，就会进行一次整理，当内部维护的六种memory region cache各自分配的内存的次数小于size，则进行free。此外当线程销毁时，netty重写了pool thread cache的finalize()方法，其中也进行了free。
